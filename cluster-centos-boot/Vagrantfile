# -*- mode: ruby -*-
# vi: set ft=ruby :
#
# author : Pramode P
#
# Pospose : Vagrant script provisions a kubernetes cluster with 1 master node and 2 worker nodes
#
# Using yaml to load external configuration files
require 'yaml'
require 'fileutils'

$init_master = <<-SCRIPT

echo "--- Executing script init_master" 

echo "--- Export variables"
HOST_NAME=$(hostname -f)
IPADDR_ENP0S8=$(ifconfig eth1 | grep inet | grep broadcast | awk '{print $2}')

echo "setup node ip in kubelet"
echo "Environment=\"KUBELET_EXTRA_ARGS=--node-ip=$IPADDR_ENP0S8\"" | sudo tee -a /usr/lib/systemd/system/kubelet.service.d/10-kubeadm.conf

echo "--- Initialise kubeadm"
sudo kubeadm init  --apiserver-advertise-address=$IPADDR_ENP0S8  --apiserver-cert-extra-sans=$IPADDR_ENP0S8  --node-name $HOST_NAME  --pod-network-cidr 10.10.0.0/16 --service-cidr  10.150.0.0/16  2>&1 | tee -a /data/$cluster/init_master.log

SCRIPT


$post_master = <<-SCRIPT

echo "--- Executing script post_master" 

echo "--- Setup kubectl for vagrant user"
sudo mkdir /root/.kube/
sudo cp /etc/kubernetes/admin.conf /root/.kube/config

echo "--- Implement Calico for Kubernetes Networking"
sudo kubectl apply -f https://docs.projectcalico.org/v3.11/manifests/calico.yaml

echo "--- Waiting for core dns pods to be up . . . "
while [ $(sudo kubectl get pods --all-namespaces | grep dns | grep Running | wc -l) != 2 ] ; do sleep 20 ; echo "--- Waiting for core dns pods to be up . . . " ; done

while [ $(sudo kubectl get nodes | grep master | grep Ready | wc -l) != 1 ] ; do sleep 20 ; echo "--- Waiting node to be ready . . . " ; done
echo "--- Matser node is Ready"
sudo kubectl get nodes

echo "--- create dummy bootstart if not exist"
[ -f /etc/kubernetes/bootstrap-kubelet.conf ] || sudo touch /etc/kubernetes/bootstrap-kubelet.conf

echo "--- Copy kube config to shared kubeadm install path"
sudo cp /etc/kubernetes/admin.conf /data/$cluster/config

echo "--- Export token for Worker Node"
sudo kubeadm token create --print-join-command > /data/$cluster/kubeadm_join_cmd.sh

SCRIPT


$init_worker = <<-SCRIPT

echo "--- Executing script init_worker"

echo "--- Join as worker node "
sudo chmod +x /data/$cluster/kubeadm_join_cmd.sh
sudo sh /data/$cluster/kubeadm_join_cmd.sh

echo "--- create dummy bootstart if not exist"
[ -f /etc/kubernetes/bootstrap-kubelet.conf ] || sudo touch /etc/kubernetes/bootstrap-kubelet.conf

SCRIPT


Vagrant.configure("2") do |config|
  # Using the hostmanager vagrant plugin to update the host files
  config.hostmanager.enabled = true
  config.hostmanager.manage_host = true
  config.hostmanager.manage_guest = true
  config.hostmanager.ignore_private_ip = false
  
  # Create a data dir to mount with in the VM information
  dirname = File.dirname("./../data/")
  unless File.directory?(dirname)
  FileUtils.mkdir_p(dirname)
  end
  
  # Loading in the VM configuration information
  servers = YAML.load_file('servers.yaml')
  
  servers.each do |servers| 
    config.vm.define servers["name"] do |srv|
      srv.vm.box = servers["box"] # Speciy the name of the Vagrant box file to use
      srv.vm.hostname = servers["name"] # Set the hostname of the VM
#     Add a second adapater with a specified IP
      srv.vm.network "private_network", ip: servers["ip"], :adapater=>2 
#	  srv.vm.network :forwarded_port, guest: 22, host: servers["port"] # Add a port forwarding rule
      srv.vm.synced_folder ".", "/vagrant", type: "virtualbox"
	  srv.vm.synced_folder "./../data/" , "/data", type: "virtualbox", owner: "root", group: "root"
	  
      srv.vm.provider "virtualbox" do |vb|
        vb.name = servers["name"] # Name of the VM in VirtualBox
        vb.cpus = servers["cpus"] # How many CPUs to allocate to the VM
        vb.memory = servers["memory"] # How much memory to allocate to the VM
#       vb.customize ["modifyvm", :id, "--cpuexecutioncap", "10"] # Limit to VM to 10% of available 
      end
  
	  if servers["name"].include? "master" then
		srv.vm.provision "shell", inline: <<-SHELL
		
		   sudo sed -i 's#\r$##g' /vagrant/install-*.sh
	       sudo chmod 755 /vagrant/install-*.sh
		   sudo cp -p /vagrant/install-*.sh /.
#          sudo sh /vagrant/install-master.sh
		   
		SHELL
	  end
	  
	  if servers["name"].include? "worker" then
	    srv.vm.provision "shell", inline: <<-SHELL
		
		   sudo sed -i 's#\r$##g' /vagrant/install-*.sh
	       sudo chmod 755 /vagrant/install-*.sh
		   sudo cp -p /vagrant/install-*.sh /.
 #          sudo sh /vagrant/install-worker.sh
		   
		SHELL
      end
	 
	end
  end
end