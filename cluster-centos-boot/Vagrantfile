# -*- mode: ruby -*-
# vi: set ft=ruby :
#
# author : Pramode P
#
# Pospose : Vagrant script provisions a kubernetes cluster with 1 master node and 2 worker nodes
#
# Using yaml to load external configuration files
require 'yaml'
require 'fileutils'



$init_master = <<-SCRIPT

echo "--- Executing script init_master" 

echo "--- pull config images"
sudo kubeadm config images pull

echo "--- Export variables"
HOST_NAME=$(hostname -f)
IPADDR_ENP0S8=$(ifconfig eth1 | grep inet | grep broadcast | awk '{print $2}')

echo "setup node ip in kubelet"
echo "Environment=\"KUBELET_EXTRA_ARGS=--node-ip=$IPADDR_ENP0S8\"" | sudo tee -a /usr/lib/systemd/system/kubelet.service.d/10-kubeadm.conf

echo "--- Initialise kubeadm"
sudo kubeadm init  --apiserver-advertise-address=$IPADDR_ENP0S8  --apiserver-cert-extra-sans=$IPADDR_ENP0S8  --node-name $HOST_NAME  --pod-network-cidr 10.10.0.0/16 --service-cidr  10.150.0.0/16  2>&1 | tee -a /data/kubebox17/init_master.log

SCRIPT


$post_master = <<-SCRIPT

echo "--- Executing script post_master" 

echo "--- Setup kubectl for vagrant user"
sudo mkdir /root/.kube/
sudo cp /etc/kubernetes/admin.conf /root/.kube/config

echo "--- Implement Calico for Kubernetes Networking"
echo "--- Projet calico : https://docs.projectcalico.org/v3.11/getting-started/kubernetes/ "
sudo kubectl apply -f https://docs.projectcalico.org/v3.11/manifests/calico.yaml

echo "--- Waiting for core dns pods to be up . . . "
#while [ $(sudo kubectl get pods --all-namespaces | grep dns | grep Running | wc -l) != 2 ] ; do sleep 20 ; echo "--- Waiting for core dns pods to be up . . . " ; done

while [ $(sudo kubectl get nodes | grep master | grep Ready | wc -l) != 1 ] ; do sleep 20 ; echo "--- Waiting node to be ready . . . " ; done
echo "--- Matser node is Ready"
sudo kubectl get nodes

echo "--- Copy kube config to shared kubeadm install path"
sudo cp /etc/kubernetes/admin.conf /data/kubebox17/config

echo "--- Export token for Worker Node"
sudo kubeadm token create --print-join-command > /data/kubebox17/kubeadm_join_cmd.sh

SCRIPT

$init_proxy = <<-SCRIPT

echo "--- Executing script init_proxy"

echo "--- Disable selinux "
sudo setenforce 0
sudo sed -i 's/^SELINUX=/#&/' /etc/selinux/config  
sudo tee -a /etc/selinux/config  << EOF
SELINUX=disabled
EOF


echo "--- Update apt and install haproxy"
sudo yum install -y haproxy
sudo systemctl enable haproxy

echo "--- Update haproxy config"
sudo tee -a /etc/haproxy/haproxy.cfg << EOF

#### Config of Ingress Traffic to Kubernetes

frontend localhost
	bind *:443
    option tcplog
    mode tcp
    default_backend nodes
backend nodes
   mode tcp
   balance roundrobin
   option ssl-hello-chk
   server node01 192.168.205.52:30001 check
   server node02 192.168.205.53:30001 check

####END of Config
EOF

echo "--- Check haproxy config status"
haproxy -f /etc/haproxy/haproxy.cfg -c -V

echo "--- Restarting haproxy service"
sudo systemctl stop haproxy
sleep 5
sudo systemctl start haproxy
sleep 
sudo systemctl status haproxy

SCRIPT

$init_nfs = <<-SCRIPT

echo "--- Executing script init_proxy"

echo "--- Login onto nfs node and install nfs server"

sudo chkconfig nfs on
sudo service rpcbind start
sudo service nfs start
sudo service nfslock start
sleep 5
sudo systemctl status nfs


echo "--- Create nfs shared directory"
sudo mkdir /nfs/kubedata -p

echo "--- Change owernship"
sudo chown nobody:nobody /nfs/kubedata

echo "--- Update exports file"
sudo tee -a /etc/exports << EOF
/nfs/kubedata    *(rw,sync,no_subtree_check,no_root_squash,no_all_squash,insecure)
EOF

sudo exportfs -a
sudo exportfs -rva

echo "--- Restar nfs server and check status"
sudo systemctl restart nfs
sudo systemctl status nfs

SCRIPT

$init_worker = <<-SCRIPT

echo "--- Executing script init_worker"

echo "--- Join as worker node "
sudo chmod +x /data/kubebox17/kubeadm_join_cmd.sh
sudo sh /data/kubebox17/kubeadm_join_cmd.sh

SCRIPT


Vagrant.configure("2") do |config|
  # Using the hostmanager vagrant plugin to update the host files
  config.hostmanager.enabled = true
  config.hostmanager.manage_host = true
  config.hostmanager.manage_guest = true
  config.hostmanager.ignore_private_ip = false
  
  # Create a data dir to mount with in the VM information
  dirname = File.dirname("./../data/")
  unless File.directory?(dirname)
  FileUtils.mkdir_p(dirname)
  end
  
  # Loading in the VM configuration information
  servers = YAML.load_file('servers.yaml')
  
  servers.each do |servers| 
    config.vm.define servers["name"] do |srv|
      srv.vm.box = servers["box"] # Speciy the name of the Vagrant box file to use
      srv.vm.hostname = servers["name"] # Set the hostname of the VM
#     Add a second adapater with a specified IP
      srv.vm.network "private_network", ip: servers["ip"], :adapater=>2 
#	  srv.vm.network :forwarded_port, guest: 22, host: servers["port"] # Add a port forwarding rule
      srv.vm.synced_folder ".", "/vagrant", type: "virtualbox"
	  srv.vm.synced_folder "./../data/" , "/data", type: "virtualbox", owner: "root", group: "root"
	  
      srv.vm.provider "virtualbox" do |vb|
        vb.name = servers["name"] # Name of the VM in VirtualBox
        vb.cpus = servers["cpus"] # How many CPUs to allocate to the VM
        vb.memory = servers["memory"] # How much memory to allocate to the VM
#       vb.customize ["modifyvm", :id, "--cpuexecutioncap", "10"] # Limit to VM to 10% of available 
      end


	  
	  if servers["name"].include? "master" then
		srv.vm.provision "shell", inline: $init_master
		srv.vm.provision "shell", inline: $post_master	
	    srv.vm.provision "shell", inline: $init_proxy
		srv.vm.provision "shell", inline: $init_nfs
	  end
	  
	  if servers["name"].include? "worker" then
	  # srv.vm.provision "shell", inline: $init_worker
     end
	 
	end
  end
end