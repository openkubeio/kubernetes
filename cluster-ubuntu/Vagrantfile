# -*- mode: ruby -*-
# vi: set ft=ruby :
#
# author : Pramode P
#
# Pospose : Vagrant script provisions a kubernetes cluster with 1 master node and 2 worker nodes
#
# Using yaml to load external configuration files
require 'yaml'

$install_docker = <<-SCRIPT
echo "--- Installing Docker ce version 17.x "
sudo apt update && sudo apt-get update
sudo apt-get install -y apt-transport-https ca-certificates curl gnupg-agent software-properties-common
curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -
sudo add-apt-repository "deb [arch=amd64] https://download.docker.com/linux/$( . /etc/os-release ; echo "$ID") $(lsb_release -cs) stable"
sudo apt-get update
sudo apt-get install -y docker-ce=$(apt-cache madison docker-ce | grep 17. | grep  ubuntu-xenial | head -1 | awk '{print $3}')
sudo tee -a /etc/docker/daemon.json  << EOF
{
  "exec-opts": ["native.cgroupdriver=systemd"],
  "log-driver": "json-file",
  "log-opts": {
    "max-size": "100m"
  },
  "storage-driver": "overlay2"
}
EOF
sudo systemctl restart docker
SCRIPT

$install_kubeadm = <<-SCRIPT
echo "--- Installing Kubeadm and kubelet - version 1.17.0"
curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add
sudo apt-add-repository "deb http://apt.kubernetes.io/ kubernetes-xenial main"
sudo apt-get update
echo "installing version `curl -s https://packages.cloud.google.com/apt/dists/kubernetes-xenial/main/binary-amd64/Packages | grep Version | grep 1.17 | awk '{print $2}'`"
sudo apt-get install kubeadm=1.17.0-00 kubelet=1.17.0-00  kubectl=1.17.0-00 -y
SCRIPT


$check_config = <<-SCRIPT
echo "--- ######### Checking Info #########"
echo "HOME=$HOME"
echo "user=`whoami`"
echo "docker  version `docker version`"
echo "kubelet version `kubelet --version`"
echo "kubeadm version `kubeadm version`"
echo "IPADDR_ENP0S8 `ifconfig enp0s8 | grep Mask | awk '{print $2}'| cut -f2 -d:`"
echo "IPADDR_ENP0S3 `ifconfig enp0s3 | grep Mask | awk '{print $2}'| cut -f2 -d:`"
echo "hostname `hostname -f`"
echo "`ip -o -4 addr list`"
SCRIPT


$init_master = <<-SCRIPT
echo "--- ######### Init Master Node #########"
echo "--- ######### Disable Swap #########"
sudo swapoff -a
sudo sed -i '/ swap /s/^\(.*\)$/#\1/g' /etc/fstab
echo "--- ######### Update Kube Config #########"
sudo sed -i 's/--bootstrap-kubeconfig=\/etc\/kubernetes\/bootstrap-kubelet.conf//' \/etc\/systemd\/system\/kubelet.service.d\/10-kubeadm.conf
echo "Environment=\"cgroup-driver=systemd\"" | sudo tee -a /etc/systemd/system/kubelet.service.d/10-kubeadm.conf
echo "--- ######### Update etc/hosts file #########"
sudo sed -i '/ip6/s/^/#/' /etc/hosts
# You may require to rum below command to ufw to allow pos to pod inter communication
# sudo ufw allow in on cni0 && sudo ufw allow out on cni0
# sudo ufw default allow routed
sudo kubeadm config images pull
HOST_NAME=$(hostname -f)
sudo mkdir -p /data/cluster-ubuntu/
IPADDR_ENP0S8=$(ifconfig enp0s8 | grep Mask | awk '{print $2}'| cut -f2 -d:)
sudo kubeadm init  --apiserver-advertise-address=$IPADDR_ENP0S8 --apiserver-cert-extra-sans=$IPADDR_ENP0S8  --node-name $(hostname -s)  --pod-network-cidr 10.10.0.0/16 --service-cidr  10.150.0.0/16  2>&1 | tee  /data/cluster-ubuntu/init_master.log
sudo kubeadm token create --print-join-command > /data/cluster-ubuntu/kubeadm_join_cmd.sh
sudo chmod +x /data/cluster-ubuntu/kubeadm_join_cmd.sh
SCRIPT


$post_master = <<-SCRIPT
echo "--- Setup kubectl and apply container networking interface"
sudo mkdir /home/vagrant/.kube/
sudo cp /etc/kubernetes/admin.conf /home/vagrant/.kube/config
sudo chown vagrant:vagrant /home/vagrant/.kube/config
echo "Implement Calico for Kubernetes Networking https://docs.projectcalico.org/v3.11/getting-started/kubernetes/ "
kubectl apply -f https://docs.projectcalico.org/v3.11/manifests/calico.yaml
while [ $(kubectl get pods --all-namespaces | grep dns | grep Running | wc -l) != 2 ] ; do sleep 20 ; echo "Waiting coredns pods to be up.... " ; done
while [ $(kubectl get nodes | grep master | grep Ready | wc -l) != 1 ] ; do sleep 20 ; echo "Waiting node to be ready0... " ; done
echo "matser node is Ready"
echo "`kubectl get nodes`"
sudo cp /etc/kubernetes/admin.conf /data/cluster-ubuntu/config
SCRIPT


$init_worker = <<-SCRIPT
echo "--- Join Worker Node
echo "--- ######### Disable Swap #########"
sudo swapoff -a
sudo sed -i '/ swap /s/^\(.*\)$/#\1/g' /etc/fstab
echo "--- ######### Update Kube Config #########"
sudo sed -i 's/--bootstrap-kubeconfig=\/etc\/kubernetes\/bootstrap-kubelet.conf//' /etc/systemd/system/kubelet.service.d/10-kubeadm.conf
echo "Environment=\"cgroup-driver=systemd\"" | sudo tee -a /etc/systemd/system/kubelet.service.d/10-kubeadm.conf
sudo sed -i '/ip6/s/^/#/' /etc/hosts
sudo sh /data/cluster-ubuntu/kubeadm_join_cmd.sh
SCRIPT


Vagrant.configure("2") do |config|
  # Using the hostmanager vagrant plugin to update the host files
  config.hostmanager.enabled = true
  config.hostmanager.manage_host = true
  config.hostmanager.manage_guest = true
  config.hostmanager.ignore_private_ip = false
  
  # Create a data dir to mount with in the VM information
  dirname = File.dirname("./../data/")
  unless File.directory?(dirname)
  FileUtils.mkdir_p(dirname)
  end
  
  # Loading in the VM configuration information
  servers = YAML.load_file('servers.yaml')
  
  servers.each do |servers| 
    config.vm.define servers["name"] do |srv|
      srv.vm.box = servers["box"] # Speciy the name of the Vagrant box file to use
      srv.vm.hostname = servers["name"] # Set the hostname of the VM
	# Add a second adapater with a specified IP
      srv.vm.network "private_network", ip: servers["ip"], :adapater=>2 
    # srv.vm.network :forwarded_port, guest: 22, host: servers["port"] # Add a port forwarding rule
      srv.vm.synced_folder ".", "/vagrant", type: "virtualbox"
	  srv.vm.synced_folder "./../data/" , "/data", type: "virtualbox", owner: "root", group: "root"

      srv.vm.provider "virtualbox" do |vb|
        vb.name = servers["name"] # Name of the VM in VirtualBox
        vb.cpus = servers["cpus"] # How many CPUs to allocate to the VM
        vb.memory = servers["memory"] # How much memory to allocate to the VM
    #   vb.customize ["modifyvm", :id, "--cpuexecutioncap", "10"]  # Limit to VM to 10% of available 
      end
	  
	  if servers["name"].include? "master" then
		srv.vm.provision "shell", inline: $install_docker
        srv.vm.provision "shell", inline: $install_kubeadm
	    srv.vm.provision "shell", inline: $check_config
		srv.vm.provision "shell", inline: $init_master
		srv.vm.provision "shell", inline: $post_master		
	  end
	  
	  if servers["name"].include? "worker" then
	    srv.vm.provision "shell", inline: $install_docker
        srv.vm.provision "shell", inline: $install_kubeadm
	    srv.vm.provision "shell", inline: $check_config
		srv.vm.provision "shell", inline: $init_worker
     end
	 
	 if servers["name"].include? "proxy" then
#	    srv.vm.provision "shell", inline: $init_proxy
     end
	end
  end
end